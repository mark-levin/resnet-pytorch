{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b96e63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T01:50:05.259241Z",
     "iopub.status.busy": "2023-12-09T01:50:05.258556Z",
     "iopub.status.idle": "2023-12-09T01:50:08.947356Z",
     "shell.execute_reply": "2023-12-09T01:50:08.946306Z"
    },
    "papermill": {
     "duration": 3.697042,
     "end_time": "2023-12-09T01:50:08.950316",
     "exception": false,
     "start_time": "2023-12-09T01:50:05.253274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f175362",
   "metadata": {
    "papermill": {
     "duration": 0.006439,
     "end_time": "2023-12-09T01:50:08.962394",
     "exception": false,
     "start_time": "2023-12-09T01:50:08.955955",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Defining ResNet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "258363e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T01:50:08.973654Z",
     "iopub.status.busy": "2023-12-09T01:50:08.972518Z",
     "iopub.status.idle": "2023-12-09T01:50:08.983904Z",
     "shell.execute_reply": "2023-12-09T01:50:08.983233Z"
    },
    "papermill": {
     "duration": 0.018879,
     "end_time": "2023-12-09T01:50:08.986220",
     "exception": false,
     "start_time": "2023-12-09T01:50:08.967341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a37c2b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T01:50:08.999085Z",
     "iopub.status.busy": "2023-12-09T01:50:08.998507Z",
     "iopub.status.idle": "2023-12-09T01:50:09.013416Z",
     "shell.execute_reply": "2023-12-09T01:50:09.012549Z"
    },
    "papermill": {
     "duration": 0.023222,
     "end_time": "2023-12-09T01:50:09.015922",
     "exception": false,
     "start_time": "2023-12-09T01:50:08.992700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c3b2b3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T01:50:09.028472Z",
     "iopub.status.busy": "2023-12-09T01:50:09.027399Z",
     "iopub.status.idle": "2023-12-09T01:50:09.042623Z",
     "shell.execute_reply": "2023-12-09T01:50:09.041844Z"
    },
    "papermill": {
     "duration": 0.024576,
     "end_time": "2023-12-09T01:50:09.044626",
     "exception": false,
     "start_time": "2023-12-09T01:50:09.020050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):  # CIFAR-10 has 10 classes\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        # Adjusted first convolutional layer for CIFAR-10\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # Max pooling layer removed for CIFAR-10\n",
    "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Adaptive pooling to handle different input sizes\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        # x = self.maxpool(x)  # Omitted for CIFAR-10\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41233b19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T01:50:09.053096Z",
     "iopub.status.busy": "2023-12-09T01:50:09.052299Z",
     "iopub.status.idle": "2023-12-09T01:50:09.059332Z",
     "shell.execute_reply": "2023-12-09T01:50:09.058510Z"
    },
    "papermill": {
     "duration": 0.013286,
     "end_time": "2023-12-09T01:50:09.061235",
     "exception": false,
     "start_time": "2023-12-09T01:50:09.047949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5b35d5",
   "metadata": {
    "papermill": {
     "duration": 0.003218,
     "end_time": "2023-12-09T01:50:09.067903",
     "exception": false,
     "start_time": "2023-12-09T01:50:09.064685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ecc7ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T01:50:09.078305Z",
     "iopub.status.busy": "2023-12-09T01:50:09.077957Z",
     "iopub.status.idle": "2023-12-09T01:50:09.087530Z",
     "shell.execute_reply": "2023-12-09T01:50:09.086828Z"
    },
    "papermill": {
     "duration": 0.015985,
     "end_time": "2023-12-09T01:50:09.089447",
     "exception": false,
     "start_time": "2023-12-09T01:50:09.073462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_cifar10():\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "554151a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T01:50:09.097368Z",
     "iopub.status.busy": "2023-12-09T01:50:09.097069Z",
     "iopub.status.idle": "2023-12-09T01:50:09.104260Z",
     "shell.execute_reply": "2023-12-09T01:50:09.103512Z"
    },
    "papermill": {
     "duration": 0.013135,
     "end_time": "2023-12-09T01:50:09.106134",
     "exception": false,
     "start_time": "2023-12-09T01:50:09.092999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, trainloader, criterion, optimizer, scheduler, device, num_epochs=200):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        scheduler.step(epoch_loss)\n",
    "            \n",
    "        end_time = time.time() \n",
    "        epoch_duration = end_time - start_time  \n",
    "        print(f\"Epoch {epoch} - Training loss: {running_loss / len(trainloader)}, Time: {epoch_duration:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb97ee11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T01:50:09.113848Z",
     "iopub.status.busy": "2023-12-09T01:50:09.113554Z",
     "iopub.status.idle": "2023-12-09T01:50:09.119259Z",
     "shell.execute_reply": "2023-12-09T01:50:09.118433Z"
    },
    "papermill": {
     "duration": 0.011542,
     "end_time": "2023-12-09T01:50:09.121080",
     "exception": false,
     "start_time": "2023-12-09T01:50:09.109538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(model, testloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6be8f3ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T01:50:09.128826Z",
     "iopub.status.busy": "2023-12-09T01:50:09.128518Z",
     "iopub.status.idle": "2023-12-09T10:17:18.016743Z",
     "shell.execute_reply": "2023-12-09T10:17:18.015436Z"
    },
    "papermill": {
     "duration": 30428.894421,
     "end_time": "2023-12-09T10:17:18.018905",
     "exception": false,
     "start_time": "2023-12-09T01:50:09.124484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:02<00:00, 58728659.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Epoch 0 - Training loss: 2.4557754295256435, Time: 153.65s\n",
      "Epoch 1 - Training loss: 1.7818801816162246, Time: 153.00s\n",
      "Epoch 2 - Training loss: 1.5140248419683608, Time: 153.55s\n",
      "Epoch 3 - Training loss: 1.259417399954613, Time: 153.62s\n",
      "Epoch 4 - Training loss: 1.0916804726928702, Time: 153.36s\n",
      "Epoch 5 - Training loss: 0.973833353241996, Time: 153.44s\n",
      "Epoch 6 - Training loss: 0.8914270674633553, Time: 153.21s\n",
      "Epoch 7 - Training loss: 0.8179510885568531, Time: 153.36s\n",
      "Epoch 8 - Training loss: 0.7735299644110453, Time: 153.28s\n",
      "Epoch 9 - Training loss: 0.7455613323870827, Time: 153.20s\n",
      "Epoch 10 - Training loss: 0.7241552117306863, Time: 153.02s\n",
      "Epoch 11 - Training loss: 0.709830514610271, Time: 153.30s\n",
      "Epoch 12 - Training loss: 0.6937271571334671, Time: 153.21s\n",
      "Epoch 13 - Training loss: 0.678151905574762, Time: 153.17s\n",
      "Epoch 14 - Training loss: 0.6695769432636783, Time: 153.01s\n",
      "Epoch 15 - Training loss: 0.6603992425114907, Time: 152.79s\n",
      "Epoch 16 - Training loss: 0.6517890810661608, Time: 152.69s\n",
      "Epoch 17 - Training loss: 0.6435136494734098, Time: 152.82s\n",
      "Epoch 18 - Training loss: 0.6397676842520609, Time: 153.14s\n",
      "Epoch 19 - Training loss: 0.6325100527699951, Time: 152.89s\n",
      "Epoch 20 - Training loss: 0.6359006737351722, Time: 152.71s\n",
      "Epoch 21 - Training loss: 0.6272519363085632, Time: 152.71s\n",
      "Epoch 22 - Training loss: 0.6230152198649428, Time: 152.90s\n",
      "Epoch 23 - Training loss: 0.610233915507641, Time: 152.91s\n",
      "Epoch 24 - Training loss: 0.6177141880211623, Time: 152.65s\n",
      "Epoch 25 - Training loss: 0.6076973237649864, Time: 152.83s\n",
      "Epoch 26 - Training loss: 0.6058506599015288, Time: 152.64s\n",
      "Epoch 27 - Training loss: 0.5991903648275854, Time: 152.66s\n",
      "Epoch 28 - Training loss: 0.5950567206305921, Time: 152.51s\n",
      "Epoch 29 - Training loss: 0.5968453842393883, Time: 152.71s\n",
      "Epoch 30 - Training loss: 0.5895337222329796, Time: 152.69s\n",
      "Epoch 31 - Training loss: 0.5954347353457184, Time: 152.65s\n",
      "Epoch 32 - Training loss: 0.5930398917182937, Time: 152.91s\n",
      "Epoch 33 - Training loss: 0.583427436516413, Time: 152.63s\n",
      "Epoch 34 - Training loss: 0.5871670213920991, Time: 152.35s\n",
      "Epoch 35 - Training loss: 0.5893344436688801, Time: 152.55s\n",
      "Epoch 36 - Training loss: 0.5810870139685738, Time: 152.29s\n",
      "Epoch 37 - Training loss: 0.586025005106426, Time: 152.28s\n",
      "Epoch 38 - Training loss: 0.5824496468619618, Time: 152.47s\n",
      "Epoch 39 - Training loss: 0.5771608830946485, Time: 152.31s\n",
      "Epoch 40 - Training loss: 0.5780422431809823, Time: 152.58s\n",
      "Epoch 41 - Training loss: 0.5759198145412118, Time: 152.12s\n",
      "Epoch 42 - Training loss: 0.573298167389677, Time: 152.41s\n",
      "Epoch 43 - Training loss: 0.5778717955817347, Time: 152.20s\n",
      "Epoch 44 - Training loss: 0.5692930455174288, Time: 152.06s\n",
      "Epoch 45 - Training loss: 0.5755428557505693, Time: 152.29s\n",
      "Epoch 46 - Training loss: 0.5722150531266351, Time: 152.37s\n",
      "Epoch 47 - Training loss: 0.5685162008037348, Time: 152.23s\n",
      "Epoch 48 - Training loss: 0.5668461967612166, Time: 152.11s\n",
      "Epoch 49 - Training loss: 0.5667135923186226, Time: 152.08s\n",
      "Epoch 50 - Training loss: 0.5695892964177729, Time: 152.11s\n",
      "Epoch 51 - Training loss: 0.5639430965151628, Time: 152.29s\n",
      "Epoch 52 - Training loss: 0.5627842500157978, Time: 152.37s\n",
      "Epoch 53 - Training loss: 0.5677267061093884, Time: 152.21s\n",
      "Epoch 54 - Training loss: 0.5666639187832927, Time: 152.20s\n",
      "Epoch 55 - Training loss: 0.5654654805846226, Time: 152.18s\n",
      "Epoch 56 - Training loss: 0.5629493505181864, Time: 152.12s\n",
      "Epoch 57 - Training loss: 0.5605924727056947, Time: 152.24s\n",
      "Epoch 58 - Training loss: 0.5589959867241437, Time: 152.27s\n",
      "Epoch 59 - Training loss: 0.5606968863807675, Time: 152.07s\n",
      "Epoch 60 - Training loss: 0.5593135682365779, Time: 151.87s\n",
      "Epoch 61 - Training loss: 0.5527055060200374, Time: 152.11s\n",
      "Epoch 62 - Training loss: 0.5612078940548251, Time: 152.21s\n",
      "Epoch 63 - Training loss: 0.5647338120757467, Time: 151.97s\n",
      "Epoch 64 - Training loss: 0.556296511550846, Time: 152.07s\n",
      "Epoch 65 - Training loss: 0.5574605853851798, Time: 151.66s\n",
      "Epoch 66 - Training loss: 0.5580332899642417, Time: 152.14s\n",
      "Epoch 67 - Training loss: 0.5505932682691632, Time: 152.04s\n",
      "Epoch 68 - Training loss: 0.5551193474465624, Time: 151.87s\n",
      "Epoch 69 - Training loss: 0.5636813907176638, Time: 151.70s\n",
      "Epoch 70 - Training loss: 0.5565170199441178, Time: 151.88s\n",
      "Epoch 71 - Training loss: 0.5518060454436581, Time: 151.99s\n",
      "Epoch 72 - Training loss: 0.5543279842190121, Time: 151.80s\n",
      "Epoch 73 - Training loss: 0.5513417244795948, Time: 151.71s\n",
      "Epoch 74 - Training loss: 0.32417171872447215, Time: 151.72s\n",
      "Epoch 75 - Training loss: 0.2611677445652311, Time: 151.79s\n",
      "Epoch 76 - Training loss: 0.23584748938908356, Time: 152.20s\n",
      "Epoch 77 - Training loss: 0.21982451307746914, Time: 151.79s\n",
      "Epoch 78 - Training loss: 0.2095170175594747, Time: 151.90s\n",
      "Epoch 79 - Training loss: 0.19689272720452464, Time: 151.97s\n",
      "Epoch 80 - Training loss: 0.19061120362271128, Time: 152.24s\n",
      "Epoch 81 - Training loss: 0.18346587380351465, Time: 151.93s\n",
      "Epoch 82 - Training loss: 0.1779360886272567, Time: 151.96s\n",
      "Epoch 83 - Training loss: 0.17226165281537245, Time: 152.17s\n",
      "Epoch 84 - Training loss: 0.17323423157472287, Time: 151.88s\n",
      "Epoch 85 - Training loss: 0.1696683271166385, Time: 151.89s\n",
      "Epoch 86 - Training loss: 0.1688045109467357, Time: 151.75s\n",
      "Epoch 87 - Training loss: 0.1679001354500461, Time: 151.88s\n",
      "Epoch 88 - Training loss: 0.16538375679193937, Time: 151.95s\n",
      "Epoch 89 - Training loss: 0.16465808258718237, Time: 151.69s\n",
      "Epoch 90 - Training loss: 0.15854490349483688, Time: 151.74s\n",
      "Epoch 91 - Training loss: 0.1589319270361415, Time: 151.99s\n",
      "Epoch 92 - Training loss: 0.16050261935776533, Time: 151.74s\n",
      "Epoch 93 - Training loss: 0.16187457804141753, Time: 151.97s\n",
      "Epoch 94 - Training loss: 0.15873977815603738, Time: 151.75s\n",
      "Epoch 95 - Training loss: 0.15678376242365982, Time: 151.74s\n",
      "Epoch 96 - Training loss: 0.15501159015337906, Time: 151.70s\n",
      "Epoch 97 - Training loss: 0.15756325348211295, Time: 151.75s\n",
      "Epoch 98 - Training loss: 0.1510236505442835, Time: 151.65s\n",
      "Epoch 99 - Training loss: 0.15214133489629267, Time: 151.86s\n",
      "Epoch 100 - Training loss: 0.14612345366269502, Time: 151.56s\n",
      "Epoch 101 - Training loss: 0.14864904376085075, Time: 151.69s\n",
      "Epoch 102 - Training loss: 0.1502541502999604, Time: 151.94s\n",
      "Epoch 103 - Training loss: 0.14477300751344552, Time: 151.80s\n",
      "Epoch 104 - Training loss: 0.14165147698586783, Time: 151.83s\n",
      "Epoch 105 - Training loss: 0.14439282968849934, Time: 151.86s\n",
      "Epoch 106 - Training loss: 0.14100782681361337, Time: 152.05s\n",
      "Epoch 107 - Training loss: 0.1399448000959328, Time: 151.65s\n",
      "Epoch 108 - Training loss: 0.13771415660943825, Time: 151.77s\n",
      "Epoch 109 - Training loss: 0.14023546828790698, Time: 151.76s\n",
      "Epoch 110 - Training loss: 0.13442120998811996, Time: 151.84s\n",
      "Epoch 111 - Training loss: 0.13862478435444442, Time: 151.59s\n",
      "Epoch 112 - Training loss: 0.1311028836261662, Time: 151.86s\n",
      "Epoch 113 - Training loss: 0.13598510363113012, Time: 152.04s\n",
      "Epoch 114 - Training loss: 0.13352806105390383, Time: 151.79s\n",
      "Epoch 115 - Training loss: 0.1294945421345208, Time: 151.72s\n",
      "Epoch 116 - Training loss: 0.1287626978605414, Time: 151.59s\n",
      "Epoch 117 - Training loss: 0.12707965347387107, Time: 151.73s\n",
      "Epoch 118 - Training loss: 0.1257540532435431, Time: 151.71s\n",
      "Epoch 119 - Training loss: 0.12727843016109733, Time: 151.80s\n",
      "Epoch 120 - Training loss: 0.1272475119880246, Time: 151.80s\n",
      "Epoch 121 - Training loss: 0.12240027631168514, Time: 151.74s\n",
      "Epoch 122 - Training loss: 0.12244163922336705, Time: 151.72s\n",
      "Epoch 123 - Training loss: 0.12523843860491882, Time: 151.82s\n",
      "Epoch 124 - Training loss: 0.12032520912392326, Time: 151.82s\n",
      "Epoch 125 - Training loss: 0.12049264337538797, Time: 152.05s\n",
      "Epoch 126 - Training loss: 0.12416097907649587, Time: 151.87s\n",
      "Epoch 127 - Training loss: 0.1146202662130794, Time: 151.78s\n",
      "Epoch 128 - Training loss: 0.12484200384058154, Time: 151.71s\n",
      "Epoch 129 - Training loss: 0.11665764615735244, Time: 151.78s\n",
      "Epoch 130 - Training loss: 0.11522034538523926, Time: 151.72s\n",
      "Epoch 131 - Training loss: 0.11889241891138999, Time: 152.01s\n",
      "Epoch 132 - Training loss: 0.11401149086163515, Time: 151.78s\n",
      "Epoch 133 - Training loss: 0.11355218263295339, Time: 152.23s\n",
      "Epoch 134 - Training loss: 0.1154604222365867, Time: 151.77s\n",
      "Epoch 135 - Training loss: 0.10983948418370369, Time: 151.97s\n",
      "Epoch 136 - Training loss: 0.11535810382650866, Time: 151.89s\n",
      "Epoch 137 - Training loss: 0.11094768647023517, Time: 151.63s\n",
      "Epoch 138 - Training loss: 0.11582957344520313, Time: 151.68s\n",
      "Epoch 139 - Training loss: 0.1145076718576767, Time: 151.92s\n",
      "Epoch 140 - Training loss: 0.10857726674993783, Time: 151.69s\n",
      "Epoch 141 - Training loss: 0.1112052777512928, Time: 151.86s\n",
      "Epoch 142 - Training loss: 0.11118593462802412, Time: 151.67s\n",
      "Epoch 143 - Training loss: 0.11322383974295329, Time: 151.76s\n",
      "Epoch 144 - Training loss: 0.1151824156727518, Time: 151.84s\n",
      "Epoch 145 - Training loss: 0.11373477146658294, Time: 151.69s\n",
      "Epoch 146 - Training loss: 0.11584932835239088, Time: 151.60s\n",
      "Epoch 147 - Training loss: 0.046796996763947864, Time: 151.54s\n",
      "Epoch 148 - Training loss: 0.026720778149304927, Time: 151.59s\n",
      "Epoch 149 - Training loss: 0.020431609580155146, Time: 151.74s\n",
      "Epoch 150 - Training loss: 0.016990198313271688, Time: 151.61s\n",
      "Epoch 151 - Training loss: 0.016069848923926072, Time: 151.86s\n",
      "Epoch 152 - Training loss: 0.01323293146374814, Time: 151.73s\n",
      "Epoch 153 - Training loss: 0.011614247749317222, Time: 151.85s\n",
      "Epoch 154 - Training loss: 0.011728128203359977, Time: 151.87s\n",
      "Epoch 155 - Training loss: 0.009851305041839713, Time: 151.94s\n",
      "Epoch 156 - Training loss: 0.010401824136268195, Time: 151.73s\n",
      "Epoch 157 - Training loss: 0.009231488960419955, Time: 151.24s\n",
      "Epoch 158 - Training loss: 0.008412081499710856, Time: 151.85s\n",
      "Epoch 159 - Training loss: 0.00842951064537246, Time: 151.89s\n",
      "Epoch 160 - Training loss: 0.006894577598354667, Time: 151.57s\n",
      "Epoch 161 - Training loss: 0.007000979554751302, Time: 152.09s\n",
      "Epoch 162 - Training loss: 0.0068427019295832885, Time: 152.01s\n",
      "Epoch 163 - Training loss: 0.00613972504877502, Time: 151.82s\n",
      "Epoch 164 - Training loss: 0.005812275901307588, Time: 151.95s\n",
      "Epoch 165 - Training loss: 0.006041625658433665, Time: 151.54s\n",
      "Epoch 166 - Training loss: 0.005854361919540902, Time: 151.73s\n",
      "Epoch 167 - Training loss: 0.005546515259523745, Time: 151.65s\n",
      "Epoch 168 - Training loss: 0.005583867969442768, Time: 151.68s\n",
      "Epoch 169 - Training loss: 0.005329773987649673, Time: 151.67s\n",
      "Epoch 170 - Training loss: 0.005462871795623501, Time: 151.80s\n",
      "Epoch 171 - Training loss: 0.004871333385502105, Time: 151.67s\n",
      "Epoch 172 - Training loss: 0.005081903958625263, Time: 151.78s\n",
      "Epoch 173 - Training loss: 0.004576570004374539, Time: 151.77s\n",
      "Epoch 174 - Training loss: 0.004292078274528465, Time: 151.86s\n",
      "Epoch 175 - Training loss: 0.004784517021856599, Time: 151.64s\n",
      "Epoch 176 - Training loss: 0.004293694317376877, Time: 151.74s\n",
      "Epoch 177 - Training loss: 0.004384962714585664, Time: 151.59s\n",
      "Epoch 178 - Training loss: 0.004093472310148365, Time: 151.65s\n",
      "Epoch 179 - Training loss: 0.003728657823150961, Time: 151.72s\n",
      "Epoch 180 - Training loss: 0.0039269755658594765, Time: 151.70s\n",
      "Epoch 181 - Training loss: 0.00411537962185893, Time: 151.51s\n",
      "Epoch 182 - Training loss: 0.004010999287940714, Time: 151.48s\n",
      "Epoch 183 - Training loss: 0.00376745088201146, Time: 151.66s\n",
      "Epoch 184 - Training loss: 0.003800481916061374, Time: 151.61s\n",
      "Epoch 185 - Training loss: 0.0033787753793028067, Time: 151.81s\n",
      "Epoch 186 - Training loss: 0.0036493585497239735, Time: 151.79s\n",
      "Epoch 187 - Training loss: 0.0035134779259293337, Time: 151.41s\n",
      "Epoch 188 - Training loss: 0.00338304212760946, Time: 151.55s\n",
      "Epoch 189 - Training loss: 0.0035822573067296457, Time: 151.47s\n",
      "Epoch 190 - Training loss: 0.0034520226700292173, Time: 151.55s\n",
      "Epoch 191 - Training loss: 0.0035654275046597184, Time: 151.56s\n",
      "Epoch 192 - Training loss: 0.003278946529155659, Time: 151.65s\n",
      "Epoch 193 - Training loss: 0.003024716148464976, Time: 151.61s\n",
      "Epoch 194 - Training loss: 0.0031930014894962014, Time: 151.77s\n",
      "Epoch 195 - Training loss: 0.0029447897577204543, Time: 151.78s\n",
      "Epoch 196 - Training loss: 0.0031583056904782264, Time: 151.63s\n",
      "Epoch 197 - Training loss: 0.003074467299505175, Time: 151.70s\n",
      "Epoch 198 - Training loss: 0.0029995868116414145, Time: 151.72s\n",
      "Epoch 199 - Training loss: 0.0032628889673787273, Time: 151.59s\n",
      "Accuracy: 94.85%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "trainloader, testloader = load_cifar10()\n",
    "\n",
    "model = ResNet50().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience=5)\n",
    "\n",
    "train(model, trainloader, criterion, optimizer, scheduler, device)\n",
    "test(model, testloader, device)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30615,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30439.024939,
   "end_time": "2023-12-09T10:17:20.797203",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-09T01:50:01.772264",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
